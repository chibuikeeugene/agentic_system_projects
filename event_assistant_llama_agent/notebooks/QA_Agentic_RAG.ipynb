{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01b6daf",
   "metadata": {},
   "source": [
    "* Project Title: Party Event-based Agentic Retrieval Augmented Generative System\n",
    "* Objective: To equip our Agentic RAG System to answer queries about our guests\n",
    "* Requirements:\n",
    "    - Our agent needs to have knowledge in this domains - sport, culture and sciencve\n",
    "    - To avoid topics (such as religion and politics) that could raise conflicts during the event\n",
    "    - The agent should have knowledge of guests background\n",
    "    - General knowledge about the weather to ensure we can find a real-time update to ensure perfect timing to launch the fireworks\n",
    "* Tools to be built:\n",
    "    - Invitees info tool - to help get an up to date info on guests\n",
    "    - Web search tool\n",
    "    - Weather tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f4160",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0281bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the needed package and class\n",
    "import datasets\n",
    "from llama_index.core.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0508eda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Ada Lovelace',\n",
       " 'relation': 'best friend',\n",
       " 'description': \"Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\",\n",
       " 'email': 'ada.lovelace@example.com'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guest_data = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "guest_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e8e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the datasset obj into Document object\n",
    "docs = [\n",
    "    Document(\n",
    "        text = '\\n'.join(\n",
    "            [\n",
    "                f\"Name: {guest_data[i]['name']}\",\n",
    "                f\"Relation: {guest_data[i]['relation']}\",\n",
    "                f\"Description: {guest_data[i]['description']}\",\n",
    "                f\"Email: {guest_data[i]['email']}\"\n",
    "            ]\n",
    "        ),\n",
    "        metadata = {'Name': f\"{guest_data[i]['name']}\"}\n",
    "    )\n",
    "    for i in range(len(guest_data))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        list\n",
      "\u001b[0;31mString form:\u001b[0m [Document(id_='fceebf0f-a406-4dd9-af61-a39efd47f300', embedding=None, metadata={'Name': 'Ada Love <...> rce=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "\u001b[0;31mLength:\u001b[0m      3\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Built-in mutable sequence.\n",
      "\n",
      "If no argument is given, the constructor creates a new empty list.\n",
      "The argument must be an iterable if specified."
     ]
    }
   ],
   "source": [
    "docs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the content of our docs\n",
    "docs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a retriever tool off our guestlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n",
    "\n",
    "def get_guest_info_retriever(query:str) -> str:\n",
    "    \"\"\"retrieve detailed information on guests based on their name or relation\"\"\"\n",
    "    result = bm25_retriever.retrieve(query)\n",
    "    logger.info(result)\n",
    "\n",
    "    if result:\n",
    "        return '\\n\\n'.join([\n",
    "            value.text for value in result[:3]\n",
    "        ])\n",
    "    else:\n",
    "        return 'No matching guest information'\n",
    "\n",
    "# create the tool\n",
    "guest_info_tool = FunctionTool.from_defaults(fn=get_guest_info_retriever)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a4073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 11:42:51.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='fceebf0f-a406-4dd9-af61-a39efd47f300', embedding=None, metadata={'Name': 'Ada Lovelace'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.847428321838379), NodeWithScore(node=TextNode(id_='e610d630-982c-4c86-849f-93a6eaa031ca', embedding=None, metadata={'Name': 'Dr. Nikola Tesla'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4113607108592987)]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ada Lovelace\n",
      "Relation: best friend\n",
      "Description: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\n",
      "Email: ada.lovelace@example.com\n",
      "\n",
      "Name: Dr. Nikola Tesla\n",
      "Relation: old friend from university days\n",
      "Description: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\n",
      "Email: nikola.tesla@gmail.com\n"
     ]
    }
   ],
   "source": [
    "print(guest_info_tool.call(\"Tell me about our guest named 'Lady Ada Lovelace'.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Party Event-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2) - 4437 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ../models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q6_K\n",
      "print_info: file size   = 6.14 GiB (6.56 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Meta Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device Metal, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 242 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  6282.97 MiB, (12767.83 / 10922.67)\n",
      "ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "load_tensors: offloading 8 repeating layers to GPU\n",
      "load_tensors: offloaded 8/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  6282.96 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  6282.97 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x16c4b1470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x164bf3320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x14120d430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x168924340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1695272e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x169525ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x169524d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x16a0e97d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x169524480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x169523d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x169522400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x1689245e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x16951f990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x16c4b1ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x144fcb990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x16c4b1d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x168924940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x168924ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x16c4b20c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x168924e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1680cd810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x144fcacc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x16a0e9a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x16a0e9c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x16c4b2320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x144fca300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x16a0e9ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x16a0ea190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1306fb760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x144fc9d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x168925110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x144fc8890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x168925470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1689256d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144fc8380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144fc7850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x168925970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x16a0ea430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x16a0ea690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x168925cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x168925f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x16c4b2640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1689261d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1689264a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x16c4b2a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1680ccb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x16a0ea960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144fc6eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x16a0eac30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x16a0eaf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x167efd6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x168926800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x168926b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x168926dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x168927020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1689272c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x16c4b2ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x16c4b3060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144fc54c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x1306f9cb0 | th_max =  384 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x144fc4bb0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144fc4530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x16c4b32c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144fc3ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x16c4b36e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144fc3540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x16c4b3940 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x16c4b3ba0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144fc2f80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144fc1cd0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x16c4b3e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x16c4b4060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x16c4b43e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x16c4b4640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144fc16c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144fc0e40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x16c4b49c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x16c4b4c20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x16c4b4e80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x16c4b5150 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x168927620 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144fc09f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x168927880 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144fbfd30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x16c4b54d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x16c4b5730 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144fbe930 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x168927be0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x16c4b5ab0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1412466a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14705b260 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x168927f40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1689281a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x16da8f8c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x16c4b5e30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x16c4b6090 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14124add0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x167efdba0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x16c4b62f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x168928400 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1206db0c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x169525db0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x16da8fb20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1680c5620 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1306fc280 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1306f8550 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x16c4b6670 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x16a0eb260 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x16c4b68d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1306f8c00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x16c4b6b30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x167eff5b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x16a0eb4c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1306f7ed0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x16c4b6eb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x16c4b7110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x16a0eb820 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x16c4b7370 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x16c4b75d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x16c4b7950 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1689286a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x168928900 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x168928c60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x167efee70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x16c504080 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x168928f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144f91bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x164b3acf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x16da8fd80 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x164bc0dc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x164bc1870 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1680b6760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x16c4b7cd0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x16a0eba80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x16c4b7f30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144ff14b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x310150f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x16c4b8190 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x16a0ebde0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x16c4b83f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x16c4b8650 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1680c4240 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x16c4b89d0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x16be61980 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x168929200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x16a0ec040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x16a0ec2a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1206d5af0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x168929500 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x16a0ec570 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1206d5d50 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1206d5fb0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x168929760 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1206d3590 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x16a0ec8d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x16a0ecb30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x16a0ecd90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x16a0ed090 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x168929ac0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x16a0ed3f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1206d7bb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x168929d90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1206d7e10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x16a0ed650 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x168929ff0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x16c4b8c30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1206d8070 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x16c5042e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x310151740 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x16c4b8e90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x16892a2c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x3101519a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x16c4b91b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x310151c00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x16892a520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x310151e60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x16c4b94d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x3101520c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x164bce4b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x164bce710 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x164bce970 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x16c4b9910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x16c4b9b70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x16892a880 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x16c4b9dd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x16892aae0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x16892adb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x164bcebd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x16892b0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x164bcee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x16892b370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x164bcf090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x16892b6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x164bcf2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x164bcf550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x16c4ba150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x16892ba30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x16c4ba3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x16a0ed8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x16892bc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x16892bef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x16892c150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x16892c420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x16c4ba610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x16892c780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x16892ca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x16c4ba990 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x164bcf7b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x164bcfa10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x16da8ffe0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x164bcfc70 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x16892ccb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x16c4babf0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x16892cf10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x16892d270 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x16c4baf70 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x16c4bb1d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x16892d5d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x16c4bb430 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x16892d830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x16892db00 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x16892ddd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x16a0edbc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x16a0ede90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x16c4bb690 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x16c4bbad0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x16c4bbf70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x16a0ee0f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x16892e0a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x16c4bc1d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x16c4bc430 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x16892e400 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x16892e660 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x16892e900 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x16892ebd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x16892eea0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x16892f170 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x164bcfed0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x164bd0130 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x164bd0390 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x164bd05f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x164bd0850 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x16c4bc690 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x16c4bc8f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x16c4bcb50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x16c4bcdb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x16892f4d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x16c4bd010 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x16c4bd270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x16da90240 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x16c4bd4d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x16892f830 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x16c4bd840 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x164bd0ab0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x16c4bdbc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x164bd0d10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x164bd0f70 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x16c4bde20 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x164bd11d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x16a0ee4a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x16a0ee770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x16a0eea40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x16892fa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x16a0eeda0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x16892fcf0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x16a0ef000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x16a0ef2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x16a0ef570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x164bd1430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x164bd1690 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x164bd18f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x164bd1b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x167efe0b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x164bd1db0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x16da904a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x16da90700 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x16c504540 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x164bd2010 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x16c5047a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x1680cde10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x1680ce070 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x1680ce2d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x16c4be1a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x16c4be400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x167efe310 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x16c4be660 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x16c504a00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x16a0ef840 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x16a0efb10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x16c4be960 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x16a0efd70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x164bd2270 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x16c4bece0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x16892ff50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x16c4bef40 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x16c4bf1a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x16a0f0130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x16c4bf400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16c4bf660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x16c4bf9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x16c4bfc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x16c4bfea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x164bd24d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1680ce530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1680ce790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x16a0f0400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1680ce9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x16a0f06d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x164bd2730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x164bd2990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1680cec50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1689301b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x168930510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x168930770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x164bd2bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x16c4c0100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x164bd2e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x16c4c0360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x16c4c05c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x164bd30b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x164bd3310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x16c4c08e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x164bd3570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x16c4c0c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x164bd37d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x164bd3a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x164bd3c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x16c4c0f20 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = Metal\n",
      "llama_kv_cache_unified: layer  25: dev = Metal\n",
      "llama_kv_cache_unified: layer  26: dev = Metal\n",
      "llama_kv_cache_unified: layer  27: dev = Metal\n",
      "llama_kv_cache_unified: layer  28: dev = Metal\n",
      "llama_kv_cache_unified: layer  29: dev = Metal\n",
      "llama_kv_cache_unified: layer  30: dev = Metal\n",
      "llama_kv_cache_unified: layer  31: dev = Metal\n",
      "llama_kv_cache_unified:      Metal KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   192.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:      Metal compute buffer size =   164.00 MiB\n",
      "llama_context:        CPU compute buffer size =   250.50 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 387 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.chunks_count': '125', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'tokenizer.ggml.model': 'gpt2', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.finetune': 'Instruct', 'general.file_type': '18', 'llama.block_count': '32', 'general.size_label': '8B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '14336', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'llama.attention.head_count': '32', 'quantize.imatrix.entries_count': '224', 'llama.context_length': '131072', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'general.name': 'Meta Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    9819.67 ms /   540 tokens (   18.18 ms per token,    54.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28435.06 ms /   255 runs   (  111.51 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   38367.21 ms /   795 tokens\n",
      "Llama.generate: 539 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4578.74 ms /   169 tokens (   27.09 ms per token,    36.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29559.13 ms /   255 runs   (  115.92 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   34257.07 ms /   424 tokens\n",
      "Llama.generate: 707 prefix-match hit, remaining 367 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    6910.38 ms /   367 tokens (   18.83 ms per token,    53.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29578.72 ms /   255 runs   (  115.99 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   36607.50 ms /   622 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred's response: \n",
      "Marie Curie is a renowned physicist and chemist, famous for her research on radioactivity.  She is not related to Lady Ada Lovelace.  However, Marie Curie has a relation with Dr. Nikola Tesla.  Dr. Nikola Tesla is an old friend from Marie Curie's university days.  He is the one who recently patented a new wireless energy transmission system and would be delighted to discuss it with you.  Just remember he's passionate about pigeons, so that might make for good small talk.  You can contact Marie Curie at marie.curie@example.com and Dr. Nikola Tesla at nikola.tesla@gmail.com. It is worth noting that the guest information retrieved is based on the name or relation provided.  It is unclear what the actual relationship between the two women might be.   Dr. Tesla's relation to Lady Ada Lovelace is not mentioned in the provided information. It is unclear if they have any relation.  You might want to ask him about that if you ever meet.  You can contact Dr. Tesla at nikola.tesla@gmail.com.  It is worth noting that the\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "# from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# initiate the model\n",
    "# llm = Ollama(model='llama2:7b', temperature=0.7)\n",
    "llm = LlamaCPP(\n",
    "    model_path=\"../models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf\",\n",
    "    temperature=0.7,\n",
    "    context_window=2048,\n",
    "    model_kwargs={\"n_gpu_layers\": 4, \"n_ctx\": 2048}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# create the agent from a function tool\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[guest_info_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# test our new agent capabilites\n",
    "output = await agent.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\")\n",
    "\n",
    "\n",
    "print(\"Alfred's response: \")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f242d",
   "metadata": {},
   "source": [
    "## Adding additional tool supports to our agent - HF hub stat tool and web search tools ( weather tool and update internet search tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6b581aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='weather_at_location(location: str) -> List[llama_index.core.schema.Document]\\n\\n        Finds the current weather at a location.\\n\\n        Args:\\n            place (str):\\n                The place to find the weather at.\\n                Should be a city name and country.\\n        ', name='weather_at_location', fn_schema=<class 'llama_index.core.tools.utils.weather_at_location'>, return_direct=False)\n",
      "ToolMetadata(description='forecast_tommorrow_at_location(location: str) -> List[llama_index.core.schema.Document]\\n\\n        Finds the weather forecast for tomorrow at a location.\\n\\n        Args:\\n            location (str):\\n                The location to find the weather tomorrow at.\\n                Should be a city name and country.\\n        ', name='forecast_tommorrow_at_location', fn_schema=<class 'llama_index.core.tools.utils.forecast_tommorrow_at_location'>, return_direct=False)\n",
      "[Document(id_='ae869773-caa0-4e5e-ab9c-39fbc6f3d385', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unable to find weather at what is the current weather in Liverpool, England.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "# weather tool \n",
    "from llama_index.tools.weather import OpenWeatherMapToolSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "open_weather_key = os.getenv('OPEN_WEATHER_API_KEY')\n",
    "\n",
    "\n",
    "weather_tool_spec = OpenWeatherMapToolSpec(key=open_weather_key)\n",
    "# create the tool\n",
    "tools = weather_tool_spec.to_tool_list()\n",
    "for tool in tools:\n",
    "    print(tool.metadata)\n",
    "\n",
    "weather_tool = weather_tool_spec.to_tool_list()[0]\n",
    "\n",
    "\n",
    "# testing out the tool\n",
    "result = weather_tool.call('what is the current weather in Liverpool, England')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c25b1de4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RatelimitException",
     "evalue": "https://links.duckduckgo.com/d.js?q=when+was+huggingface+platform+made+public%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-144989448283941718767879249954094752024&bing_market=wt-WT&ex=-1 202 Ratelimit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRatelimitException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# create function tool \u001b[39;00m\n\u001b[1;32m      8\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m FunctionTool\u001b[38;5;241m.\u001b[39mfrom_defaults(fn\u001b[38;5;241m=\u001b[39msearch_tool_spec\u001b[38;5;241m.\u001b[39mduckduckgo_full_search)\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msearch_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhen was huggingface platform made public?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:323\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    326\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/tools/function_tool.py:249\u001b[0m, in \u001b[0;36mFunctionTool.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ToolOutput:\n\u001b[1;32m    248\u001b[0m     all_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:323\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    326\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/tools/function_tool.py:258\u001b[0m, in \u001b[0;36mFunctionTool.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx_param_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_kwargs:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext is required for this tool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Exclude the Context param from the tool output so that the Context can be serialized\u001b[39;00m\n\u001b[1;32m    261\u001b[0m tool_output_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    262\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx_param_name\n\u001b[1;32m    263\u001b[0m }\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/tools/duckduckgo/base.py:53\u001b[0m, in \u001b[0;36mDuckDuckGoSearchToolSpec.duckduckgo_full_search\u001b[0;34m(self, query, region, max_results)\u001b[0m\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m: region,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_results,\n\u001b[1;32m     50\u001b[0m }\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddg:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mddg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/duckduckgo_search/duckduckgo_search.py:246\u001b[0m, in \u001b[0;36mDDGS.text\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    243\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml is not installed. Using backend=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    248\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_html(keywords, region, timelimit, max_results)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/duckduckgo_search/duckduckgo_search.py:333\u001b[0m, in \u001b[0;36mDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    331\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(r)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(islice(results, max_results))\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/duckduckgo_search/duckduckgo_search.py:330\u001b[0m, in \u001b[0;36mDDGS._text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    328\u001b[0m     slist\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, max_results, \u001b[38;5;241m15\u001b[39m))\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(_text_api_page, slist):\n\u001b[1;32m    331\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(r)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.22_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.22_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.22_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.22_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/duckduckgo_search/duckduckgo_search.py:308\u001b[0m, in \u001b[0;36mDDGS._text_api.<locals>._text_api_page\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_text_api_page\u001b[39m(s: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    307\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 308\u001b[0m     resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://links.duckduckgo.com/d.js\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     page_data \u001b[38;5;241m=\u001b[39m _text_extract_json(resp_content, keywords)\n\u001b[1;32m    310\u001b[0m     page_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/duckduckgo_search/duckduckgo_search.py:136\u001b[0m, in \u001b[0;36mDDGS._get_url\u001b[0;34m(self, method, url, params, content, data)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_event\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m202\u001b[39m, \u001b[38;5;241m301\u001b[39m, \u001b[38;5;241m403\u001b[39m):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RatelimitException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Ratelimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m return None. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRatelimitException\u001b[0m: https://links.duckduckgo.com/d.js?q=when+was+huggingface+platform+made+public%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-144989448283941718767879249954094752024&bing_market=wt-WT&ex=-1 202 Ratelimit"
     ]
    }
   ],
   "source": [
    "# search tool\n",
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "\n",
    "# instantiate the DuckduckGo search tool\n",
    "search_tool_spec = DuckDuckGoSearchToolSpec()\n",
    "\n",
    "# create function tool \n",
    "search_tool = FunctionTool.from_defaults(fn=search_tool_spec.duckduckgo_full_search)\n",
    "response = search_tool('when was huggingface platform made public?')\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "004fbe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esm2_t30_150M_UR50D with 21,657,160 downloads\n"
     ]
    }
   ],
   "source": [
    "# model stats tool - \n",
    "# a tool to fetch model statistics from the Hugging Face Hub based on a username.\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "# defining a get_model_stats funtion\n",
    "def get_model_stats(author:str) -> str:\n",
    "    \"\"\" fetch the most downloaded model from a specific author\"\"\"\n",
    "    try:\n",
    "        models = list(list_models(author=author, sort='downloads', direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f'The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads'\n",
    "        else:\n",
    "            return f'No model found for {author}'\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f'Could not fetch models for {author} due to {str(e)}'\n",
    "    \n",
    "# creating the tool\n",
    "model_stats_tool = FunctionTool.from_defaults(fn=get_model_stats)\n",
    "\n",
    "result = model_stats_tool('facebook')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a00c70",
   "metadata": {},
   "source": [
    "## Building the agents with all the necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ad78f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 91 prefix-match hit, remaining 864 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =   14887.01 ms /   864 tokens (   17.23 ms per token,    58.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28089.12 ms /   255 runs   (  110.15 ms per token,     9.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   43099.75 ms /  1119 tokens\n",
      "Llama.generate: 954 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    3005.25 ms /    75 tokens (   40.07 ms per token,    24.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26537.61 ms /   255 runs   (  104.07 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   29646.66 ms /   330 tokens\n",
      "Llama.generate: 1028 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    5455.55 ms /   237 tokens (   23.02 ms per token,    43.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26675.32 ms /   255 runs   (  104.61 ms per token,     9.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   32464.92 ms /   492 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred's response:  %s Ada Lovelace was a mathematician and is often credited as the first computer programmer. She worked with Charles Babbage on the Analytical Engine. She is also known for her work in mathematics and computing. She is the best friend of Marie Curie who was a physicist and chemist. She is known for her work on radioactivity and was the first woman to win a Nobel Prize.  Thought: I can answer the question without using any more tools. I'll use the user's language to answer\n",
      "Answer: Ada Lovelace was a mathematician and is often credited as the first computer programmer. She worked with Charles Babbage on the Analytical Engine. She is also known for her work in mathematics and computing. She is the best friend of Marie Curie who was a physicist and chemist. She is known for her work on radioactivity and was the first woman to win a Nobel Prize.  Thought: I can answer the question without using any more tools. I'll use the user's language to answer\n",
      "Answer:  I think there is an error in the above response, as Ada Lovelace and Marie Curie\n"
     ]
    }
   ],
   "source": [
    "party_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[model_stats_tool, search_tool, weather_tool, guest_info_tool],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "response = await party_agent.run('who is Ada lovelace and what is facebook most downloaded model?')\n",
    "print(\"Alfred's response:  %s\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 959 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   29973.50 ms /   256 runs   (  117.08 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   30091.59 ms /   257 tokens\n",
      "Llama.generate: 959 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    3192.91 ms /    71 tokens (   44.97 ms per token,    22.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27005.14 ms /   255 runs   (  105.90 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   30316.91 ms /   326 tokens\n",
      "Llama.generate: 1029 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    5270.53 ms /   219 tokens (   24.07 ms per token,    41.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27412.08 ms /   255 runs   (  107.50 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   32790.17 ms /   474 tokens\n",
      "Llama.generate: 1247 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4519.16 ms /   183 tokens (   24.69 ms per token,    40.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27891.24 ms /   255 runs   (  109.38 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   32519.56 ms /   438 tokens\n",
      "Llama.generate: 1429 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    5445.56 ms /   228 tokens (   23.88 ms per token,    41.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28628.07 ms /   255 runs   (  112.27 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   34192.25 ms /   483 tokens\n",
      "Llama.generate: 1656 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    6663.54 ms /   273 tokens (   24.41 ms per token,    40.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13847.18 ms /   118 runs   (  117.35 ms per token,     8.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   20574.44 ms /   391 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred's response: \n",
      "\n",
      "The current weather in New York, USA is clear sky with a wind speed of 5.14 m/s and a direction of 270°. The humidity is 40% and the temperature is 27.19°C with a high of 27.98°C and a low of 25.57°C. The feels like temperature is 27.0°C. There is no rain and the cloud cover is 0%. \n",
      "assistant: Thought: The current language of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-05 01:20:39.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='e610d630-982c-4c86-849f-93a6eaa031ca', embedding=None, metadata={'Name': 'Dr. Nikola Tesla'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.9424822330474854), NodeWithScore(node=TextNode(id_='24de1642-cb8f-4fec-9905-1f0dc25db99e', embedding=None, metadata={'Name': 'Marie Curie'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Name: Marie Curie\\nRelation: no relation\\nDescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nEmail: marie.curie@example.com', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.0)]\u001b[0m\n",
      "\u001b[32m2025-07-05 01:22:25.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='e610d630-982c-4c86-849f-93a6eaa031ca', embedding=None, metadata={'Name': 'Dr. Nikola Tesla'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.9424822330474854), NodeWithScore(node=TextNode(id_='24de1642-cb8f-4fec-9905-1f0dc25db99e', embedding=None, metadata={'Name': 'Marie Curie'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Name: Marie Curie\\nRelation: no relation\\nDescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nEmail: marie.curie@example.com', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.0)]\u001b[0m\n",
      "\u001b[32m2025-07-05 01:26:42.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='fceebf0f-a406-4dd9-af61-a39efd47f300', embedding=None, metadata={'Name': 'Ada Lovelace'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.3980638980865479), NodeWithScore(node=TextNode(id_='24de1642-cb8f-4fec-9905-1f0dc25db99e', embedding=None, metadata={'Name': 'Marie Curie'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Name: Marie Curie\\nRelation: no relation\\nDescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nEmail: marie.curie@example.com', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.0)]\u001b[0m\n",
      "\u001b[32m2025-07-05 01:27:20.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='fceebf0f-a406-4dd9-af61-a39efd47f300', embedding=None, metadata={'Name': 'Ada Lovelace'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=3.3212456703186035), NodeWithScore(node=TextNode(id_='e610d630-982c-4c86-849f-93a6eaa031ca', embedding=None, metadata={'Name': 'Dr. Nikola Tesla'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.2431708425283432)]\u001b[0m\n",
      "\u001b[32m2025-07-05 01:28:35.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_guest_info_retriever\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m[NodeWithScore(node=TextNode(id_='fceebf0f-a406-4dd9-af61-a39efd47f300', embedding=None, metadata={'Name': 'Ada Lovelace'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=3.3212456703186035), NodeWithScore(node=TextNode(id_='e610d630-982c-4c86-849f-93a6eaa031ca', embedding=None, metadata={'Name': 'Dr. Nikola Tesla'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7506330609321594)]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# asking our agent a new question\n",
    "result = await party_agent.run('one of our guest is from google, what can you tell me about their most popular model')\n",
    "print(\"Alfred's response: \\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d777c",
   "metadata": {},
   "source": [
    "# Notice the above result isn't grounded and lacks relevance to the question asked. In summary the agent hallucinated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae1ee2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 938 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    1782.27 ms /    29 tokens (   61.46 ms per token,    16.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30725.33 ms /   255 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   32640.14 ms /   284 tokens\n",
      "Llama.generate: 966 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4455.84 ms /   172 tokens (   25.91 ms per token,    38.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30521.03 ms /   255 runs   (  119.69 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   35110.93 ms /   427 tokens\n",
      "Llama.generate: 1137 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4405.95 ms /   142 tokens (   31.03 ms per token,    32.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31380.12 ms /   255 runs   (  123.06 ms per token,     8.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   35966.87 ms /   397 tokens\n",
      "Llama.generate: 1278 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4944.65 ms /   147 tokens (   33.64 ms per token,    29.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29239.67 ms /   255 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   34308.23 ms /   402 tokens\n",
      "Llama.generate: 1424 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4525.61 ms /   172 tokens (   26.31 ms per token,    38.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29509.57 ms /   255 runs   (  115.72 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   34156.80 ms /   427 tokens\n",
      "Llama.generate: 1595 prefix-match hit, remaining 423 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    9507.35 ms /   423 tokens (   22.48 ms per token,    44.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3675.31 ms /    29 runs   (  126.73 ms per token,     7.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   13208.72 ms /   452 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred's response: \n",
      "\n",
      " Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: weather_at_location\n"
     ]
    }
   ],
   "source": [
    "# asking our agent a new question - combining multiple tool calls\n",
    "text = \"\"\"\n",
    "I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\n",
    "\"\"\"\n",
    "result = await party_agent.run(text)\n",
    "print(\"Alfred's response: \\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cded4e",
   "metadata": {},
   "source": [
    "## Keeping context between via conversation memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6ff8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 944 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   27130.19 ms /   256 runs   (  105.98 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   27241.08 ms /   257 tokens\n",
      "Llama.generate: 946 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4057.35 ms /   154 tokens (   26.35 ms per token,    37.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28704.23 ms /   255 runs   (  112.57 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   32874.88 ms /   409 tokens\n",
      "Llama.generate: 946 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4937.07 ms /   207 tokens (   23.85 ms per token,    41.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29282.22 ms /   255 runs   (  114.83 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   34333.46 ms /   462 tokens\n",
      "Llama.generate: 946 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    5252.77 ms /   251 tokens (   20.93 ms per token,    47.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30085.60 ms /   255 runs   (  117.98 ms per token,     8.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   35481.69 ms /   506 tokens\n",
      "Llama.generate: 946 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    6008.86 ms /   294 tokens (   20.44 ms per token,    48.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26979.12 ms /   255 runs   (  105.80 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   33092.05 ms /   549 tokens\n",
      "Llama.generate: 1239 prefix-match hit, remaining 156 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    9820.52 ms\n",
      "llama_perf_context_print: prompt eval time =    4693.84 ms /   156 tokens (   30.09 ms per token,    33.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33441.16 ms /   255 runs   (  131.14 ms per token,     7.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   38295.23 ms /   411 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that Ada Lovelace is a historical figure known for her contributions to mathematics and computing. Unfortunately, I couldn't find any information on the most downloaded model from her author. The tool returned a message indicating that no model was found for her. However, it's worth noting that the tool was not able to find any information on a model from an unknown author either.  If you could provide more context or information, I might be able to help you better.  Also, it's worth mentioning that the tool returned a description of Ada Lovelace from the get_guest_info_retriever tool, which seems to be a mistake, as it's not the correct description of her.  I hope this answer is helpful, please let me know if you have any other questions.  If you would like me to answer any other questions, please let me know and I will do my best to assist you.  Thank you for using this tool.  I hope you found the information you were looking for.  If you have any other questions, feel free to ask!  I'll do my best to assist you.  Good luck with your\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "cts =  Context(party_agent)\n",
    "\n",
    "response = await party_agent.run('who is Ada?', ctx=cts)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecomm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
