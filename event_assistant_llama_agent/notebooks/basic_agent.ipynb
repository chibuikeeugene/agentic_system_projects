{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261448c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the relevant libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bd86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN =  os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181f515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings, but thanks for asking! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# implementing a basic text completion\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model =  \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 200,\n",
    "    token = HF_TOKEN,\n",
    ")\n",
    "\n",
    "output = llm.complete('How are you doing?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34dad7",
   "metadata": {},
   "source": [
    "## Creating a RAG Pipeline using Components from LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ee0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader # directory reader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # to create our vector representation\n",
    "from llama_index.core.node_parser import SentenceSplitter # split document into chunks (nodes)\n",
    "from llama_index.core.ingestion import IngestionPipeline # hanles transformation and loading into the vectorstore\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac6ae2",
   "metadata": {},
   "source": [
    "## Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a846fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 83.24it/s]\n"
     ]
    }
   ],
   "source": [
    "reader =  SimpleDirectoryReader(input_dir='./data') # load the reader object\n",
    "\n",
    "docs = reader.load_data(show_progress=True) # load the data and return the list of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7936f",
   "metadata": {},
   "source": [
    "## Instantiating a DB client and creating our ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c03e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "Generating embeddings: 100%|██████████| 5120/5120 [00:21<00:00, 242.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating our db client and then the vectorstore\n",
    "client =  chromadb.PersistentClient()\n",
    "chroma_collection =  client.get_or_create_collection('llama_collection')\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection) # now we have our vector store object\n",
    "\n",
    "\n",
    "# using the ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "transformations=[\n",
    "    SentenceSplitter(chunk_size=100, chunk_overlap=0),\n",
    "    HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "],\n",
    "vector_store=vector_store\n",
    ")\n",
    "\n",
    "# creating the vector embeddings - injesting directly into the vectordb\n",
    "nodes  = pipeline.run(\n",
    "    show_progress = True,\n",
    "    documents = docs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d084b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an index from our vectorstore and its embeddings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# creating the index object\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4b9ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided information does not contain any details about the writers of a journal. The given text appears to be a file path and some non-readable characters, which do not provide any information about authors or contributors to a journal.\n"
     ]
    }
   ],
   "source": [
    "# Before we can query our index, we need to convert it to a query interface. The most common conversion options are:\n",
    "\n",
    "# as_retriever: For basic document retrieval, returning a list of NodeWithScore objects with similarity scores\n",
    "# as_query_engine: For single question-answer interactions, returning a written response\n",
    "# as_chat_engine: For conversational interactions that maintain memory across multiple messages, \n",
    "# returning a written response using chat history and indexed context\n",
    "\n",
    "\n",
    "# let's query our index\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode = \"tree_summarize\" # other options - \"refine\" and \"compact\"\n",
    ")\n",
    "\n",
    "response  =  query_engine.query('list the writers of the journal')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26224243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the query engine response\n",
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "\n",
    "evaluator = CorrectnessEvaluator(\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "eval_result = evaluator.evaluate_response(\n",
    "    query='list the writers of the journal',\n",
    "    response=response)\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7025bb",
   "metadata": {},
   "source": [
    "## Tooling - creating tools for Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb860648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a function tool\n",
    "# import the package\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e8732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='8', tool_name='calculator', raw_input={'args': (3, 5), 'kwargs': {}}, raw_output=8, is_error=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a basic calculator function\n",
    "def add(x:int, y:int) -> int:\n",
    "    \"\"\"calculate the sum of two variables\"\"\"\n",
    "    total =  x + y\n",
    "    return total\n",
    "\n",
    "# convert it to a tool using the functiontool class\n",
    "tool = FunctionTool.from_defaults(\n",
    "    add,\n",
    "    'calculator',\n",
    "    'calculate the sum of two variables'\n",
    ")\n",
    "\n",
    "tool.call(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490930d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The provided information does not contain any details about the writers of a journal. The data shown appears to be a file path and some encoded or non-readable text, which does not include author names or any other information related to the writers of a journal.', tool_name='science-journal-engine', raw_input={'input': 'list the writers of the journal'}, raw_output=Response(response='The provided information does not contain any details about the writers of a journal. The data shown appears to be a file path and some encoded or non-readable text, which does not include author names or any other information related to the writers of a journal.', source_nodes=[NodeWithScore(node=TextNode(id_='787f5dda-a049-4468-a709-312a59a2fb18', embedding=None, metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='384840d2-edd5-4eca-8eed-6b225ff136e8', node_type='4', metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, hash='e93369222b8ca23a012157f6dbbb7d0b910ece6216c1e7887cf178953fd6dd2c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1c287c8e-2b0e-4839-a06e-301cb505c761', node_type='1', metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, hash='cd7ee98bcd7a68342e3bf89e09ff28591366c1b44be21130420d0f6ad187a0f4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9a8946d1-8be0-42af-907c-aec2aac54367', node_type='1', metadata={}, hash='3f9adc142ad64605d8a36b5029d6e24427f0aa990c5ee80a47f00c1248063c2c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='ݜ\\x08Ĝ{j(&{O\\x06<9\\x12#XW\\x1d3\\n\\t>\\x05atㄱ]R`)\\x01Zt#3}fΓ<|h+T>^\\x04k\\x17\\x0e)|=J\\u0558\\tW\\x15\\x05wI', mimetype='text/plain', start_char_idx=45935, end_char_idx=45994, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.38354129623149397), NodeWithScore(node=TextNode(id_='e92f3868-3cc0-494d-ad88-04f7d14f20db', embedding=None, metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2a97eba0-f9eb-4341-92f3-8ffe9dfd48c7', node_type='4', metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, hash='e93369222b8ca23a012157f6dbbb7d0b910ece6216c1e7887cf178953fd6dd2c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fd14f371-9162-4e4c-9a86-f8420a5fdcf0', node_type='1', metadata={'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, hash='cd7ee98bcd7a68342e3bf89e09ff28591366c1b44be21130420d0f6ad187a0f4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3fd58201-334a-4cac-8c87-bb0d9718dd72', node_type='1', metadata={}, hash='3f9adc142ad64605d8a36b5029d6e24427f0aa990c5ee80a47f00c1248063c2c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='ݜ\\x08Ĝ{j(&{O\\x06<9\\x12#XW\\x1d3\\n\\t>\\x05atㄱ]R`)\\x01Zt#3}fΓ<|h+T>^\\x04k\\x17\\x0e)|=J\\u0558\\tW\\x15\\x05wI', mimetype='text/plain', start_char_idx=45935, end_char_idx=45994, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.38354129623149397)], metadata={'787f5dda-a049-4468-a709-312a59a2fb18': {'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}, 'e92f3868-3cc0-494d-ad88-04f7d14f20db': {'file_path': '/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/llama_agent/data/A132113010125.pdf', 'file_name': 'A132113010125.pdf', 'file_type': 'application/pdf', 'file_size': 548575, 'creation_date': '2025-06-09', 'last_modified_date': '2025-01-17'}}), is_error=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing a basic queryengine tool\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name='science-journal-engine',\n",
    "    description='a knowledge base to find local information about a scientific journal'\n",
    ")\n",
    "\n",
    "await tool.acall('list the writers of the journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4218601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\n\\n        Searches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query (str): The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n\\n        \"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\n\\n        Create and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n\\n        \"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\n\\n        Update a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\n\\n        Get a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\n\\n        Sends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n\\n        \")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing a tool spec\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "# convert it to a list of tools\n",
    "tool_spec_list  = tool_spec.to_tool_list()\n",
    "\n",
    "# view the meta data of each tool in the list\n",
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf05e95",
   "metadata": {},
   "source": [
    "## Agents - creating agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a basic function-calling agent\n",
    "\n",
    "# firstly, define a function - perhaps a calculator\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\" calculate the multiplication of two numbers of type integers\"\"\"\n",
    "    return a*b\n",
    "\n",
    "# define your llm\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# Initialize the agent using the AgentWorkFlow class \n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "agent  = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb673c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 times 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "# testing the Function calling agent\n",
    "result =  await agent.run('what is 2 times 2')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob, it seems you asked for a multiplication, and 2 multiplied by 3 is 6. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "# implementing a stateful function-calling agent\n",
    "# to remember or keep state of past interactions we can use the Context class \n",
    "\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "#wrap the context around the agent\n",
    "ctx = Context(agent)\n",
    "\n",
    "# re-writing our agent run method\n",
    "result1 = await agent.run('My name is bob', ctx=ctx)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name \"Bob\" is capitalized. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "# test out our initial agent runs\n",
    "result2 = await agent.run('was my name capitalize?', ctx=ctx)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81f306",
   "metadata": {},
   "source": [
    "### Creating a more complex agent - Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d90a7b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/workflow/context.py:628\u001b[0m, in \u001b[0;36mContext._step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 628\u001b[0m     new_ev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    629\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:369\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:399\u001b[0m, in \u001b[0;36mAgentWorkflow.run_agent_step\u001b[0;34m(self, ctx, ev)\u001b[0m\n\u001b[1;32m    397\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tools(ev\u001b[38;5;241m.\u001b[39mcurrent_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m agent_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mtake_step(\n\u001b[1;32m    400\u001b[0m     ctx,\n\u001b[1;32m    401\u001b[0m     ev\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m    402\u001b[0m     tools,\n\u001b[1;32m    403\u001b[0m     memory,\n\u001b[1;32m    404\u001b[0m )\n\u001b[1;32m    406\u001b[0m ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(agent_output)\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/agent/workflow/react_agent.py:101\u001b[0m, in \u001b[0;36mReActAgent.take_step\u001b[0;34m(self, ctx, llm_input, tools, memory)\u001b[0m\n\u001b[1;32m    100\u001b[0m last_chat_response \u001b[38;5;241m=\u001b[39m ChatResponse(message\u001b[38;5;241m=\u001b[39mChatMessage())\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m    102\u001b[0m     raw \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m         last_chat_response\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response\u001b[38;5;241m.\u001b[39mraw, BaseModel)\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m    106\u001b[0m     )\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/llms/huggingface_api/base.py:462\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m cur_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mchat_completion(\n\u001b[1;32m    463\u001b[0m     messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_huggingface_messages(messages),\n\u001b[1;32m    464\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfinish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/huggingface_hub/inference/_generated/_async_client.py:964\u001b[0m, in \u001b[0;36mAsyncInferenceClient.chat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    957\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[1;32m    958\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    959\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[1;32m    963\u001b[0m )\n\u001b[0;32m--> 964\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_post(request_parameters, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/huggingface_hub/inference/_generated/_async_client.py:290\u001b[0m, in \u001b[0;36mAsyncInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m session\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/huggingface_hub/inference/_generated/_async_client.py:276\u001b[0m, in \u001b[0;36mAsyncInferenceClient._inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/aiohttp/client_reqrep.py:1161\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1164\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1165\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1166\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1167\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWorkflowRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# next initialize the agent\u001b[39;00m\n\u001b[1;32m     16\u001b[0m query_engine_agent \u001b[38;5;241m=\u001b[39m AgentWorkflow\u001b[38;5;241m.\u001b[39mfrom_tools_or_functions(\n\u001b[1;32m     17\u001b[0m     tools_or_functions\u001b[38;5;241m=\u001b[39m[query_engine_tool],\n\u001b[1;32m     18\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     19\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou are a kind assistant with access to a database of journal information\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m query_response \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mawait\u001b[39;00m query_engine_agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho are the authors of the journal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_response)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.22_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py:287\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper.<locals>.handle_future_result\u001b[0;34m(future, span_id, bound_args, instance, context)\u001b[0m\n\u001b[1;32m    285\u001b[0m exception \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mexception()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[1;32m    289\u001b[0m result \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_exit(\n\u001b[1;32m    291\u001b[0m     id_\u001b[38;5;241m=\u001b[39mspan_id,\n\u001b[1;32m    292\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[1;32m    293\u001b[0m     instance\u001b[38;5;241m=\u001b[39minstance,\n\u001b[1;32m    294\u001b[0m     result\u001b[38;5;241m=\u001b[39mresult,\n\u001b[1;32m    295\u001b[0m )\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/workflow/workflow.py:408\u001b[0m, in \u001b[0;36mWorkflow.run.<locals>._run_workflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mwrite_event_to_stream(StopEvent())\n",
      "File \u001b[0;32m~/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/llama_index/core/workflow/context.py:637\u001b[0m, in \u001b[0;36mContext._step_worker\u001b[0;34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager, dispatcher)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mretry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[1;32m    638\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in step \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     delay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mretry_policy\u001b[38;5;241m.\u001b[39mnext(\n\u001b[1;32m    642\u001b[0m         retry_start_at \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(), attempts, e\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[0;31mWorkflowRuntimeError\u001b[0m: Error in step 'run_agent_step': 402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "# first we can re-use the intial index created earlier\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# next, create a tool of this engine\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name='knowledge base tool',\n",
    "    description='searches and retrieve local information about the journal',\n",
    "    return_direct=False\n",
    ")\n",
    "\n",
    "# next initialize the agent\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt= 'you are a kind assistant with access to a database of journal information',\n",
    ")\n",
    "\n",
    "query_response =  await query_engine_agent.run('who are the authors of the journal')\n",
    "print(query_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6eef3",
   "metadata": {},
   "source": [
    "### Creating a Multi-agentic system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chibuikeiwuchukwu/Docs/Real_ML_Project/ecommerce_system/ecomm_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 152.51it/s]\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "Generating embeddings: 100%|██████████| 326/326 [30:49<00:00,  5.67s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 119\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# testing our multi-agent system\u001b[39;00m\n\u001b[1;32m    115\u001b[0m oiutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent_response\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    116\u001b[0m     user_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho are the authors of the journal?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# importing the necessary packages\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.agent.workflow import ReActAgent, AgentWorkflow\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool, FunctionTool\n",
    "from llama_index.core import SimpleDirectoryReader # directory reader\n",
    "from llama_index.core.node_parser import SentenceSplitter # split document into chunks (nodes)\n",
    "from llama_index.core.ingestion import IngestionPipeline # hanles transformation and loading into the vectorstore\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "\n",
    "# selecting and instantiating a llm model\n",
    "llm = Ollama(\n",
    "    model='deepseek-r1',\n",
    "    request_timeout=60.0,\n",
    "    temperature=0.7,\n",
    "    context_window=8000\n",
    ")\n",
    "\n",
    "# create a rag system and convert it to query-engine tool\n",
    "# step1: using the simpledirectoryreader to load the file in a director\n",
    "reader = SimpleDirectoryReader('./data')\n",
    "docs = reader.load_data(show_progress=True)\n",
    "\n",
    "\n",
    "# step 2: split into tokens, convert to embeddings and save in the vector using the Ingestion Pipeline\n",
    "# creating new client\n",
    "client =  chromadb.PersistentClient()\n",
    "chroma_collection =  client.get_or_create_collection('journal_collection')\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection) # now we have our vector store object\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0, ),\n",
    "        OllamaEmbedding(model_name='deepseek-r1')\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    show_progress = True,\n",
    "    documents = docs,\n",
    "    )\n",
    "\n",
    "# instantiating an embedding model\n",
    "embed_model = OllamaEmbedding(model_name='deepseek-r1')\n",
    "\n",
    "\n",
    "# creating an index\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "\n",
    "query_engine_index = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode = \"tree_summarize\"\n",
    ")\n",
    "\n",
    "\n",
    "# creating a simple basic calculator function\n",
    "def add(x:int, y:int) -> int:\n",
    "    \"\"\"add two numbers together\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "\n",
    "# creating a 2 ReAct agents \n",
    "\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine_index,\n",
    "    name='query engine tool',\n",
    "    description='queries a rag system '\n",
    ")\n",
    "\n",
    "\n",
    "# creating a function tool\n",
    "calculator_tool = FunctionTool.from_defaults(\n",
    "    fn=add,\n",
    "    name='a basic calculator',\n",
    "    description='adds two numbers together'\n",
    ")\n",
    "\n",
    "\n",
    "#ReAct Agent 1\n",
    "query_engine_agent = ReActAgent(\n",
    "    name='journal lookup',\n",
    "    description='looks up information in relation to the journal article',\n",
    "    system_prompt='Use this tool to query a RAG system to retrieve information to related questions',\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm\n",
    "\n",
    ")\n",
    "\n",
    "#ReAct Agent 2\n",
    "calculator_agent = ReActAgent(\n",
    "    name='basic calculator',\n",
    "    description='adds two numbers',\n",
    "    system_prompt='Use this tool to compute addition of numbers',\n",
    "    tools=[calculator_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "agent_response = AgentWorkflow(\n",
    "    agents= [query_engine_agent, calculator_agent],\n",
    "    root_agent='basic calculator'\n",
    ")\n",
    "\n",
    "# testing our multi-agent system\n",
    "output = await agent_response.run(\n",
    "    user_msg='who are the authors of the journal?'\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "175e2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so looking at the conversation history, the user initially asked about the authors of a journal, which I couldn't answer directly. So, I used the handoff tool, directing them to an agent called 'journal lookup'. \n",
      "\n",
      "Now, the observation says that the agent has taken over and is handling the request. My task now is to figure out how to proceed with this new information.\n",
      "\n",
      "I need to understand what exactly the 'journal lookup' agent does. From the previous message, it seems like they handle accessing journal author information. But since I don't have access to specific data, maybe they can only provide general info or perhaps require more details from the user.\n",
      "\n",
      "So the next step is probably for me to ask a follow-up question to get the necessary information. The most logical way would be to ask what specific journal they're interested in. That way, once I know the journal name, I can pass that information to the 'journal lookup' agent again or perhaps another tool if needed.\n",
      "\n",
      "I should structure my response clearly, asking for the journal name and specifying it in a structured format so that the next step is straightforward.\n",
      "</think>\n",
      "\n",
      "Thought: The user needs more specific information about a particular journal. I'll ask them to provide the journal's name to get accurate author details.\n",
      "Action Input: {\"input\": \"Which journal are you referring to?\"}\n"
     ]
    }
   ],
   "source": [
    "# testing our multi-agent system\n",
    "output = await agent_response.run(\n",
    "    user_msg='who are the authors of the journal?'\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0914c00",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0acf533",
   "metadata": {},
   "source": [
    "### Getting started workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halleluia\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from llama_index.core.workflow import StartEvent, StopEvent, step, Event, Workflow\n",
    "\n",
    "\n",
    "class MyWorkFlow(Workflow):\n",
    "    \"\"\"basic single step workflow\"\"\"\n",
    "    @step\n",
    "    async def first_step(self, ev:StartEvent) -> StopEvent:\n",
    "        return StopEvent(result='Halleluia')\n",
    "    \n",
    "\n",
    "# testing out our basic workflow\n",
    "# instantiate the object \n",
    "mfw = MyWorkFlow()\n",
    "result = await mfw.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15dd15f",
   "metadata": {},
   "source": [
    "### Multi-step workflow implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7846d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine ending after Machine starting...\n"
     ]
    }
   ],
   "source": [
    "# To connect multiple steps, we create custom events that carry data between steps.\n",
    "\n",
    "# start by defining an intermediate event which is custom-defined\n",
    "class IntermediateEvent(Event):\n",
    "    intermediate_output: str\n",
    "\n",
    "# define your workflow\n",
    "class VendingMachineWorflow(Workflow):\n",
    "    \"\"\" handles the vending machine work processes\"\"\"\n",
    "    @step\n",
    "    def first_step(self, ev:StartEvent) -> IntermediateEvent:\n",
    "        return IntermediateEvent(intermediate_output= 'Machine starting...')\n",
    "    \n",
    "    @step\n",
    "    def second_step(self, ev: IntermediateEvent) -> StopEvent:\n",
    "        final_answer = f'Machine ending after {ev.intermediate_output}'\n",
    "        return StopEvent(result=final_answer)\n",
    "    \n",
    "# creating the vending machine workflow instance\n",
    "vending_flow =  VendingMachineWorflow()\n",
    "vending_result = await vending_flow.run()\n",
    "print(vending_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb21ce3",
   "metadata": {},
   "source": [
    "### Adding loops, joins and branches to our multi-step agentic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f67ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting a loop in our multi-step workflow\n",
    "import random as re\n",
    "import typing as t\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result:str\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_result: str\n",
    "\n",
    "class MyMultiStepWorkflow(Workflow):\n",
    "    \"\"\" impelemting a looping agentic workflow\"\"\"\n",
    "\n",
    "    @step\n",
    "    async def first_step(self, ev: t.Union[StartEvent, LoopEvent]) -> t.Union[ProcessingEvent, LoopEvent]:\n",
    "        if re.randint(0,10) == 0:\n",
    "            print('try again...')\n",
    "            return LoopEvent(loop_result='you need to try again')\n",
    "        else:\n",
    "            return ProcessingEvent(intermediate_result='you made it, carry on to step two')\n",
    "    \n",
    "    @step\n",
    "    async def second_step(self, ev:ProcessingEvent) -> StopEvent:\n",
    "        final_output =  f'{ev.intermediate_result}, Now you have arrive.'\n",
    "        return StopEvent(result=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26fbdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try again...\n",
      "you made it, carry on to step two, Now you have arrive.\n"
     ]
    }
   ],
   "source": [
    "# testing out new agentic workflow\n",
    "looping_agent = MyMultiStepWorkflow()\n",
    "result = await looping_agent.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73769bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.html\n"
     ]
    }
   ],
   "source": [
    "# visualizing the workflow\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(looping_agent, 'flow.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149ac7c",
   "metadata": {},
   "source": [
    "### Using state management in our agentic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0686d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting the argument ctx which will be of type Context and help keep track of the state of our workflow\n",
    "import random as re\n",
    "import typing as t\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result:str\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_result: str\n",
    "\n",
    "class MyMultiStepWorkflow(Workflow):\n",
    "    \"\"\" impelemting a looping agentic workflow\"\"\"\n",
    "\n",
    "    @step\n",
    "    async def first_step(self, ctx:Context, ev: t.Union[StartEvent, LoopEvent]) -> t.Union[ProcessingEvent, LoopEvent]:\n",
    "        if re.randint(0,10) == 0:\n",
    "            print('try again...')\n",
    "            await ctx.set('outcome', 'you need to try again' )\n",
    "            return LoopEvent(loop_result='you need to try again')\n",
    "            \n",
    "        else:\n",
    "            await ctx.set('outcome', 'you made it, carry on to step two' )\n",
    "            return ProcessingEvent(intermediate_result='you made it, carry on to step two')\n",
    "            \n",
    "    \n",
    "    @step\n",
    "    async def second_step(self, ctx:Context,  ev:ProcessingEvent) -> StopEvent:\n",
    "        final_output =  f'{ev.intermediate_result}, Now you have arrive.'\n",
    "        current_context = await ctx.get('outcome')\n",
    "        print(current_context) # confirming state validity\n",
    "        return StopEvent(result=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41f5de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you made it, carry on to step two\n",
      "you made it, carry on to step two, Now you have arrive.\n"
     ]
    }
   ],
   "source": [
    "stateful_agent = MyMultiStepWorkflow()\n",
    "result = await stateful_agent.run()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecomm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
